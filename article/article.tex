% !TEX root = ./article.tex

\documentclass{article}

\usepackage{mystyle}
\usepackage{myvars}



%-----------------------------

\begin{document}

	\maketitle % Insert title

	\thispagestyle{fancy} % All pages have headers and footers


%-----------------------------
%	ABSTRACT
%-----------------------------

	\begin{abstract}
		\noindent [TODO ]
	\end{abstract}

%-----------------------------
%	TEXT
%-----------------------------


	\section{Introducción}
	\label{sec:introducción}

		\paragraph{}
		[TODO]

	\section{La figura \ref{e1:plot} muestra un conjunto de entrenamiento con ejemplos positivos (estrellas) y negativos (círculos). Se desea clasificar la nueva instancia $<3,3>$ mediante el algoritmo $K$-vecinos más próximos. Obtener la clasificación para los valores de $K=\{ 1, 3, 5\}$ utilizando las distancias indicadas a continuación}
	\label{sec:e1}

		\paragraph{}
		[TODO]

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.5\textwidth]{exercise-1-plot}
			\end{center}
			\caption{Representación Gráfica del problema \ref{sec:e1}}
			\label{e1:plot}
		\end{figure}

		\begin{equation}
		\label{eq:estrellas}
			ESTRELLAS = \{(1,1), (1,4), (3,1), (5,3)\}
		\end{equation}

		\begin{equation}
		\label{eq:circulos}
			CIRCULOS  = \{(2,1), (5,2), (6,1)\}
		\end{equation}

		\begin{equation}
		\label{eq:instancia}
			instancia  = (3,3)
		\end{equation}

		$max_x = 6$ $min_x = 1$ $max_y = 4$ $min_y = 1$


		\begin{equation}
		\label{eq:normalization}
			normalize(x,y) = (\frac{x-min_x}{max_x-min_x},\frac{y-min_y}{max_y-min_y})
		\end{equation}


		\begin{equation}
		\label{eq:estrellas_normalized}
			ESTRELLAS_{normalized} = \{(0,0), (0,1), (\tfrac{2}{5},0), (\tfrac{4}{5},\tfrac{2}{3})\}
		\end{equation}

		\begin{equation}
		\label{eq:circulos_normalized}
			CIRCULOS_{normalized}  = \{(\tfrac{1}{5},0), (\tfrac{4}{5},\tfrac{1}{3}), (1,0)\}
		\end{equation}

		\begin{equation}
		\label{eq:instancia_normalized}
			instancia_{normalized}  = (\tfrac{2}{5},\tfrac{2}{3})
		\end{equation}

		\subsection{Distancia Euclídea}


			\begin{equation}
				D_{euclidean}(a,b) = \sqrt{(a_x - b_x)^2 + (a_y - b_y)^2}
			\end{equation}

			\begin{equation}
				R_{euclidean} = \{4.242_e, 3.605_e, 3.969_e, 3.206_e, 4.103_c, 3.457_c, 3.605_c\}
			\end{equation}

			\paragraph{}
			[TODO]

			\begin{equation}
				min(R_{euclidean},1) = \{ 3.206_e \}
			\end{equation}

			\begin{equation}
				min(R_{euclidean},3) = \{ 3.206_e, 3.457_c, 3.605_e\}
			\end{equation}

			\paragraph{}
			Empate

			\begin{equation}
				min(R_{euclidean},5) = \{ 3.206_e, 3.457_c, 3.605_e, 3.605_c,  3.969_e \}
			\end{equation}

		\subsection{Distancia Euclídea Ponderada: $w_x=0.2, w_y=0.8$}

			\paragraph{}
			[TODO]

			\begin{equation}
				D_{w_{euclidean}}(a,b) = \sqrt{w_x(a_x - b_x)^2 + w_y(a_y - b_y)^2} = \sqrt{0.2(a_x - b_x)^2 + 0.8(a_y - b_y)^2}
			\end{equation}

			\begin{equation}
				R_{w_{euclidean}} = \{3.0_e, 2.236_e, 2.924_e, 2.307_e, 2.961_c, 2.580_c, 2.828_c\}
			\end{equation}

			\begin{equation}
				min(R_{w_{euclidean}},1) = \{ 2.236_e \}
			\end{equation}

			\begin{equation}
				min(R_{w_{euclidean}},3) = \{ 2.236_e, 2.307_e, 2.580_c\}
			\end{equation}

			\begin{equation}
				min(R_{w_{euclidean}},5) = \{ 2.236_e, 2.307_e, 2.580_c, 2.828_c, 2.924_e\}
			\end{equation}


		\subsection{Distancia de Manhattan}

			\paragraph{}
			[TODO]

			\begin{equation}
				D_{manhattan}(a,b) = |a_x - b_x| + |a_y - b_y|
			\end{equation}

			\begin{equation}
				R_{manhattan} = \{4_e, 3_e, 2_e, 2_e, 3_c, 3_c, 5_c\}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},1) = \{ 2_e \}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},3) = \{ 2_e, 2_e, 3_e\}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},5) = \{ 2_e, 2_e, 3_e, 3_c, 3_c, \}
			\end{equation}

		\subsection{Distancia de Hamming}

			\paragraph{}
			[TODO]

			\begin{equation}
				D_{hamming}(a,b) = (a_x \neq b_x) + (a_y \neq b_y)
			\end{equation}

			\begin{equation}
				R_{hamming} = \{2_e, 2_e, 1_e, 1_e, 2_c, 2_c, 2_c\}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},1) = \{ 1_e \}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},3) = \{ 1_e, 1_e, 2_e\}
			\end{equation}

			\begin{equation}
				min(R_{manhattan},5) = \{ 1_e, 1_e, 2_e, 3_c, 3_c, \}
			\end{equation}

	\section{Dígitos manuscritos}

		\paragraph{}
		[TODO]

		\begin{table}[h]
			\centering
			\small
			\begin{tabu}{ | c | c | c | c | c | c | c | c | c | c | }
				\hline
				\multicolumn{10}{ | c | }{Validación cruzada de 10 particiones --- $K$-Vecinos más Próximos} \\ \hline
				\multirow{2}{*}{Datos}	& \multicolumn{9}{ c |}{Tasa de Error ($K=$)} \\ \cline{2-10}
																& \emph{1} & \emph{2} & \emph{3} & \emph{4} & \emph{5} & \emph{6} & \emph{7} & \emph{8}	& \emph{9}\\ \hline
				Entrenae						& $3.448\%$	 & $3.524\%$ & $3.065\%$ & $3.141\%$	& $3.371\%$ & $3.218\%$	 & $3.295\%$ & $3.295\%$ & $3.371\%$	\\
				\hline
			\end{tabu}
			\caption{Tasa de error obtenida tras realizar un experimento de Validación cruzada de 10 particiones con el clasificador \emph{K-NN} para $k \in \{1,2,...,9\}$}
			\label{table:e2}
		\end{table}



		\begin{figure}[h]
			\begin{center}
				\begin{tikzpicture}
					\begin{axis}[
						% only scale the axis, not the axis including the ticks and labels
		        scale only axis=true,
		        % set `width' and `height' to the desired values
		        width=0.9\textwidth,
		        height=0.3\textwidth,
						]
						\addplot table [x=k, y=error, col sep=comma] {data.csv};
					\end{axis}
				\end{tikzpicture}
			\end{center}
			\caption{Representación Gráfica de la tasa de error obtenida tras realizar un experimento de Validación cruzada de 10 particiones con el clasificador \emph{K-NN} para $k \in \{1,2,...,9\}$}
			\label{plot:e2}
		\end{figure}

		\paragraph{}
		[TODO]


%-----------------------------
%	Bibliographic references
%-----------------------------
	\nocite{garciparedes:machine-learning-instance-based}
	\nocite{subject:taa}
	\nocite{tool:weka}
  \bibliographystyle{alpha}
  \bibliography{bib/misc}

\end{document}
